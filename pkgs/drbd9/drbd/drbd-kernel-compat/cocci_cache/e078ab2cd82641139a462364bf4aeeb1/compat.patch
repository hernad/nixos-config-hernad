--- ./drbd_int.h
+++ /tmp/cocci-output-1018120-4a4e6f-drbd_int.h
@@ -443,7 +443,7 @@ struct drbd_peer_request {
 	};
 
 	struct drbd_page_chain_head page_chain;
-	blk_opf_t opf; /* to be used as bi_opf */
+	unsigned int opf; /* to be used as bi_opf */
 	atomic_t pending_bios;
 	struct drbd_interval i;
 	unsigned long flags;	/* see comments on ee flag bits below */
@@ -1960,7 +1960,7 @@ extern void do_submit(struct work_struct
 #define __drbd_make_request(d,b,k,j) __drbd_make_request(d,b,j)
 #endif
 extern void __drbd_make_request(struct drbd_device *, struct bio *, ktime_t, unsigned long);
-extern void drbd_submit_bio(struct bio *bio);
+extern blk_qc_t drbd_submit_bio(struct bio *bio);
 
 enum drbd_force_detach_flags {
 	DRBD_READ_ERROR,
@@ -2025,7 +2025,8 @@ extern void verify_progress(struct drbd_
 extern void *drbd_md_get_buffer(struct drbd_device *device, const char *intent);
 extern void drbd_md_put_buffer(struct drbd_device *device);
 extern int drbd_md_sync_page_io(struct drbd_device *device,
-		struct drbd_backing_dev *bdev, sector_t sector, enum req_op op);
+		struct drbd_backing_dev *bdev, sector_t sector,
+		unsigned int op);
 extern bool drbd_al_active(struct drbd_device *device, sector_t sector, unsigned int size);
 extern void drbd_ov_out_of_sync_found(struct drbd_peer_device *, sector_t, int);
 extern void wait_until_done_or_force_detached(struct drbd_device *device,
--- drbd_transport_tcp.c
+++ /tmp/cocci-output-1018120-d64382-drbd_transport_tcp.c
@@ -646,7 +646,7 @@ static int dtt_wait_for_connect(struct d
 	rcu_read_unlock();
 
 	timeo = connect_int * HZ;
-	timeo += get_random_u32_below(2) ? timeo / 7 : -timeo / 7; /* 28.5% random jitter */
+	timeo += (prandom_u32() % 2) ? timeo / 7 : -timeo / 7; /* 28.5% random jitter */
 
 retry:
 	timeo = wait_event_interruptible_timeout(listener->wait,
@@ -1112,7 +1112,7 @@ retry:
 				kernel_sock_shutdown(s, SHUT_RDWR);
 				sock_release(s);
 randomize:
-				if (get_random_u32_below(2))
+				if ((prandom_u32() % 2))
 					goto retry;
 			}
 		}
@@ -1134,9 +1134,6 @@ randomize:
 	dsocket->sk->sk_allocation = GFP_NOIO;
 	csocket->sk->sk_allocation = GFP_NOIO;
 
-	dsocket->sk->sk_use_task_frag = false;
-	csocket->sk->sk_use_task_frag = false;
-
 	dsocket->sk->sk_priority = TC_PRIO_INTERACTIVE_BULK;
 	csocket->sk->sk_priority = TC_PRIO_INTERACTIVE;
 
--- drbd_state.c
+++ /tmp/cocci-output-1018120-32e995-drbd_state.c
@@ -4611,7 +4611,7 @@ long twopc_retry_timeout(struct drbd_res
 			retries = 5;
 		timeout = resource->res_opts.twopc_retry_timeout *
 			  HZ / 10 * connections * (1 << retries);
-		timeout = get_random_u32_below(timeout);
+		timeout = (prandom_u32() % timeout);
 	}
 	return timeout;
 }
@@ -4791,7 +4791,7 @@ change_cluster_wide_state(bool (*change)
 	}
 
 	do
-		reply->tid = get_random_u32();
+		reply->tid = prandom_u32();
 	while (!reply->tid);
 
 	request.tid = reply->tid;
@@ -5023,7 +5023,7 @@ retry:
 	*reply = (struct twopc_reply) { 0 };
 
 	do
-		reply->tid = get_random_u32();
+		reply->tid = prandom_u32();
 	while (!reply->tid);
 
 	request.tid = reply->tid;
--- drbd_sender.c
+++ /tmp/cocci-output-1018120-f342e7-drbd_sender.c
@@ -323,7 +323,7 @@ void drbd_request_endio(struct bio *bio)
 
 	/* to avoid recursion in __req_mod */
 	if (unlikely(status)) {
-		enum req_op op = bio_op(bio);
+		unsigned int op = bio_op(bio);
 		if (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {
 			if (status == BLK_STS_NOTSUPP)
 				what = DISCARD_COMPLETED_NOTSUPP;
--- drbd_req.c
+++ /tmp/cocci-output-1018120-4d7d88-drbd_req.c
@@ -1411,7 +1411,7 @@ static bool remote_due_to_read_balancing
 		/* originally, this used the bdi congestion framework,
 		 * but that was removed in linux 5.18.
 		 * so just never report the lower device as congested. */
-		return false;
+		return bdi_read_congested(device->ldev->backing_bdev->bd_disk->bdi);
 	case RB_LEAST_PENDING:
 		return atomic_read(&device->local_cnt) >
 			atomic_read(&peer_device->ap_pending_cnt) + atomic_read(&peer_device->rs_pending_cnt);
@@ -1809,7 +1809,8 @@ drbd_request_prepare(struct drbd_device
 	req->start_jif = bio_start_io_acct(req->master_bio);
 
 	if (get_ldev(device)) {
-		req->private_bio = bio_alloc_clone(device->ldev->backing_bdev, bio, GFP_NOIO, &drbd_io_bio_set);
+		req->private_bio = bio_clone_fast(bio, GFP_NOIO,
+						  &drbd_io_bio_set);
 		req->private_bio->bi_private = req;
 		req->private_bio->bi_end_io = drbd_request_endio;
 	}
@@ -2560,7 +2561,7 @@ static bool request_size_bad(struct drbd
  *                                           v
  *                                   Request state machine
  */
-void drbd_submit_bio(struct bio *bio)
+blk_qc_t drbd_submit_bio(struct bio *bio)
 {
 	struct drbd_device *device = bio->bi_bdev->bd_disk->private_data;
 #ifdef CONFIG_DRBD_TIMING_STATS
@@ -2571,17 +2572,15 @@ void drbd_submit_bio(struct bio *bio)
 	if (drbd_fail_request_early(device, bio)) {
 		bio->bi_status = BLK_STS_IOERR;
 		bio_endio(bio);
-		return;
+		return BLK_QC_T_NONE;
 	}
 
-	bio = bio_split_to_limits(bio);
-	if (!bio)
-		return;
+	blk_queue_split(&bio);
 
 	if (device->cached_err_io || request_size_bad(device, bio)) {
 		bio->bi_status = BLK_STS_IOERR;
 		bio_endio(bio);
-		return;
+		return BLK_QC_T_NONE;
 	}
 
 	/* This is both an optimization: READ of size 0, nothing to do
@@ -2593,13 +2592,14 @@ void drbd_submit_bio(struct bio *bio)
 	if (bio_op(bio) == REQ_OP_READ && bio->bi_iter.bi_size == 0) {
 		WARN_ONCE(1, "size zero read from upper layers");
 		bio_endio(bio);
-		return;
+		return BLK_QC_T_NONE;
 	}
 
 	ktime_get_accounting(start_kt);
 	start_jif = jiffies;
 
 	__drbd_make_request(device, bio, start_kt, start_jif);
+	return BLK_QC_T_NONE;
 }
 
 static unsigned long time_min_in_future(unsigned long now,
--- drbd_receiver.c
+++ /tmp/cocci-output-1018120-cc1ee2-drbd_receiver.c
@@ -1354,8 +1354,7 @@ static void one_flush_endio(struct bio *
 
 static void submit_one_flush(struct drbd_device *device, struct issue_flush_context *ctx)
 {
-	struct bio *bio = bio_alloc(device->ldev->backing_bdev, 0,
-			REQ_OP_WRITE | REQ_PREFLUSH, GFP_NOIO);
+	struct bio *bio = bio_alloc(GFP_NOIO, 0);
 	struct one_flush_context *octx = kmalloc(sizeof(*octx), GFP_NOIO);
 
 	if (!octx) {
@@ -1374,12 +1373,14 @@ static void submit_one_flush(struct drbd
 
 	octx->device = device;
 	octx->ctx = ctx;
+	bio_set_dev(bio, device->ldev->backing_bdev);
 	bio->bi_private = octx;
 	bio->bi_end_io = one_flush_endio;
 
 	device->flush_jif = jiffies;
 	set_bit(FLUSH_PENDING, &device->flags);
 	atomic_inc(&ctx->pending);
+	bio->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
 	submit_bio(bio);
 }
 
@@ -1722,7 +1723,8 @@ int drbd_issue_discard_or_zero_out(struc
 	granularity = max(q->limits.discard_granularity >> 9, 1U);
 	alignment = (bdev_discard_alignment(bdev) >> 9) % granularity;
 
-	max_discard_sectors = min(bdev_max_discard_sectors(bdev), (1U << 22));
+	max_discard_sectors = min(bdev_get_queue(bdev)->limits.max_discard_sectors,
+				  (1U << 22));
 	max_discard_sectors -= max_discard_sectors % granularity;
 	if (unlikely(!max_discard_sectors))
 		goto zero_out;
@@ -1746,7 +1748,8 @@ int drbd_issue_discard_or_zero_out(struc
 		start = tmp;
 	}
 	while (nr_sectors >= max_discard_sectors) {
-		err |= blkdev_issue_discard(bdev, start, max_discard_sectors, GFP_NOIO);
+		err |= blkdev_issue_discard(bdev, start, max_discard_sectors,
+					    GFP_NOIO, 0);
 		nr_sectors -= max_discard_sectors;
 		start += max_discard_sectors;
 	}
@@ -1758,7 +1761,8 @@ int drbd_issue_discard_or_zero_out(struc
 		nr = nr_sectors;
 		nr -= (unsigned int)nr % granularity;
 		if (nr) {
-			err |= blkdev_issue_discard(bdev, start, nr, GFP_NOIO);
+			err |= blkdev_issue_discard(bdev, start, nr, GFP_NOIO,
+						    0);
 			nr_sectors -= nr;
 			start += nr;
 		}
@@ -1776,7 +1780,7 @@ static bool can_do_reliable_discards(str
 	struct disk_conf *dc;
 	bool can_do;
 
-	if (!bdev_max_discard_sectors(device->ldev->backing_bdev))
+	if (!bdev_get_queue(device->ldev->backing_bdev)->limits.max_discard_sectors)
 		return false;
 
 	rcu_read_lock();
@@ -1882,12 +1886,13 @@ next_bio:
 
 	/* we special case some flags in the multi-bio case, see below
 	 * (REQ_PREFLUSH, or BIO_RW_BARRIER in older kernels) */
-	bio = bio_alloc(device->ldev->backing_bdev, nr_pages, peer_req->opf,
-			GFP_NOIO);
+	bio = bio_alloc(GFP_NOIO, nr_pages);
+	bio_set_dev(bio, device->ldev->backing_bdev);
 	/* > peer_req->i.sector, unless this is the first bio */
 	bio->bi_iter.bi_sector = sector;
 	bio->bi_private = peer_req;
 	bio->bi_end_io = drbd_peer_request_endio;
+	bio->bi_opf = peer_req->opf;
 
 	bio->bi_next = bios;
 	bios = bio;
@@ -3074,7 +3079,7 @@ static int wait_for_and_update_peer_seq(
 	return ret;
 }
 
-static enum req_op wire_flags_to_bio_op(u32 dpf)
+static unsigned int wire_flags_to_bio_op(u32 dpf)
 {
 	if (dpf & DP_ZEROES)
 		return REQ_OP_WRITE_ZEROES;
@@ -3084,9 +3089,9 @@ static enum req_op wire_flags_to_bio_op(
 }
 
 /* see also bio_flags_to_wire() */
-static blk_opf_t wire_flags_to_bio(struct drbd_connection *connection, u32 dpf)
+static unsigned int wire_flags_to_bio(struct drbd_connection *connection, u32 dpf)
 {
-	blk_opf_t opf = wire_flags_to_bio_op(dpf) |
+	unsigned int opf = wire_flags_to_bio_op(dpf) |
 		(dpf & DP_RW_SYNC ? REQ_SYNC : 0);
 
 	/* we used to communicate one bit only in older DRBD */
@@ -5488,7 +5493,7 @@ static int receive_protocol(struct drbd_
 		drbd_info(connection, "peer data-integrity-alg: %s\n",
 			  integrity_alg[0] ? integrity_alg : "(none)");
 
-	kvfree_rcu_mightsleep(old_net_conf);
+	kvfree_rcu(old_net_conf);
 	return 0;
 
 disconnect_rcu_unlock:
@@ -5949,7 +5954,7 @@ static int receive_sizes(struct drbd_con
 			new_disk_conf->disk_size = p_usize;
 
 			rcu_assign_pointer(device->ldev->disk_conf, new_disk_conf);
-			kvfree_rcu_mightsleep(old_disk_conf);
+			kvfree_rcu(old_disk_conf);
 
 			drbd_info(peer_device, "Peer sets u_size to %llu sectors (old: %llu)\n",
 				 (unsigned long long)p_usize, (unsigned long long)my_usize);
@@ -7191,7 +7196,7 @@ drbd_commit_size_change(struct drbd_devi
 		new_disk_conf->disk_size = tr->user_size;
 
 		rcu_assign_pointer(device->ldev->disk_conf, new_disk_conf);
-		kvfree_rcu_mightsleep(old_disk_conf);
+		kvfree_rcu(old_disk_conf);
 
 		drbd_info(device, "New u_size %llu sectors\n",
 			  (unsigned long long)tr->user_size);
@@ -8981,8 +8986,8 @@ void drbd_process_rs_discards(struct drb
 		 * than DRBD_MAX_RS_DISCARD_SIZE, then allow merging up to a size of
 		 * DRBD_MAX_RS_DISCARD_SIZE.
 		 */
-		align = max(DRBD_MAX_RS_DISCARD_SIZE, bdev_discard_granularity(
-					device->ldev->backing_bdev)) >> SECTOR_SHIFT;
+		align = max(DRBD_MAX_RS_DISCARD_SIZE,
+		            (device->ldev->backing_bdev->bd_disk->queue->limits.discard_granularity ? : 512)) >> SECTOR_SHIFT;
 		put_ldev(device);
 	}
 
--- drbd_nl.c
+++ /tmp/cocci-output-1018120-47ec40-drbd_nl.c
@@ -1945,7 +1945,7 @@ static void decide_on_discard_support(st
 	struct request_queue *q = device->rq_queue;
 	unsigned int max_discard_sectors;
 
-	if (bdev && !bdev_max_discard_sectors(bdev->backing_bdev))
+	if (bdev && !bdev_get_queue(bdev->backing_bdev)->limits.max_discard_sectors)
 		goto not_supported;
 
 	if (!(common_connection_features(device->resource) & DRBD_FF_TRIM)) {
@@ -1963,6 +1963,7 @@ static void decide_on_discard_support(st
 	 * topology on all peers.
 	 */
 	blk_queue_discard_granularity(q, 512);
+	blk_queue_flag_set(QUEUE_FLAG_DISCARD, q);
 	max_discard_sectors = drbd_max_discard_sectors(device->resource);
 	blk_queue_max_discard_sectors(q, max_discard_sectors);
 	blk_queue_max_write_zeroes_sectors(q, max_discard_sectors);
@@ -1970,6 +1971,7 @@ static void decide_on_discard_support(st
 
 not_supported:
 	blk_queue_discard_granularity(q, 0);
+	blk_queue_flag_clear(QUEUE_FLAG_DISCARD, q);
 	blk_queue_max_discard_sectors(q, 0);
 }
 
@@ -1995,6 +1997,7 @@ static void fixup_discard_support(struct
 
 	if (discard_granularity > max_discard) {
 		blk_queue_discard_granularity(q, 0);
+		blk_queue_flag_clear(QUEUE_FLAG_DISCARD, q);
 		blk_queue_max_discard_sectors(q, 0);
 	}
 }
@@ -2040,6 +2043,7 @@ void drbd_reconsider_queue_parameters(st
 	}
 	q->limits = common_limits;
 	blk_queue_max_hw_sectors(q, common_limits.max_hw_sectors);
+	blk_queue_max_write_same_sectors(q, 0);
 	decide_on_discard_support(device, bdev);
 
 	fixup_write_zeroes(device, q);
@@ -2134,7 +2138,7 @@ static void sanitize_disk_conf(struct dr
 	if (disk_conf->al_extents > drbd_al_extents_max(nbc))
 		disk_conf->al_extents = drbd_al_extents_max(nbc);
 
-	if (!bdev_max_discard_sectors(bdev)) {
+	if (!bdev_get_queue(bdev)->limits.max_discard_sectors) {
 		if (disk_conf->rs_discard_granularity) {
 			disk_conf->rs_discard_granularity = 0; /* disable feature */
 			drbd_info(device, "rs_discard_granularity feature disabled\n");
@@ -2152,8 +2156,8 @@ static void sanitize_disk_conf(struct dr
 	if (disk_conf->rs_discard_granularity) {
 		unsigned int new_discard_granularity =
 			disk_conf->rs_discard_granularity;
-		unsigned int discard_sectors = bdev_max_discard_sectors(bdev);
-		unsigned int discard_granularity = bdev_discard_granularity(bdev);
+		unsigned int discard_sectors = bdev_get_queue(bdev)->limits.max_discard_sectors;
+		unsigned int discard_granularity = (bdev->bd_disk->queue->limits.discard_granularity ? : 512);
 
 		/* should be at least the discard_granularity of the bdev,
 		 * and preferably a multiple (or the backend won't be able to
@@ -2302,7 +2306,7 @@ int drbd_adm_disk_opts(struct sk_buff *s
 			drbd_send_sync_param(peer_device);
 	}
 
-	kvfree_rcu_mightsleep(old_disk_conf);
+	kvfree_rcu(old_disk_conf);
 	mod_timer(&device->request_timer, jiffies + HZ);
 	goto success;
 
@@ -3727,7 +3731,7 @@ int drbd_adm_net_opts(struct sk_buff *sk
 
 	mutex_unlock(&connection->mutex[DATA_STREAM]);
 	mutex_unlock(&connection->resource->conf_update);
-	kvfree_rcu_mightsleep(old_net_conf);
+	kvfree_rcu(old_net_conf);
 
 	if (connection->cstate[NOW] >= C_CONNECTED) {
 		struct drbd_peer_device *peer_device;
@@ -3859,7 +3863,7 @@ int drbd_adm_peer_device_opts(struct sk_
 
 	rcu_assign_pointer(peer_device->conf, new_peer_device_conf);
 
-	kvfree_rcu_mightsleep(old_peer_device_conf);
+	kvfree_rcu(old_peer_device_conf);
 	kfree(old_plan);
 
 	/* No need to call drbd_send_sync_param() here. The values in
@@ -4879,7 +4883,7 @@ int drbd_adm_resize(struct sk_buff *skb,
 		new_disk_conf->disk_size = (sector_t)rs.resize_size;
 		rcu_assign_pointer(device->ldev->disk_conf, new_disk_conf);
 		mutex_unlock(&device->resource->conf_update);
-		kvfree_rcu_mightsleep(old_disk_conf);
+		kvfree_rcu(old_disk_conf);
 		new_disk_conf = NULL;
 	}
 
@@ -5489,7 +5493,8 @@ static void device_to_statistics(struct
 		/* originally, this used the bdi congestion framework,
 		 * but that was removed in linux 5.18.
 		 * so just never report the lower device as congested. */
-		s->dev_lower_blocked = false;
+		s->dev_lower_blocked = bdi_congested(device->ldev->backing_bdev->bd_disk->bdi,
+						     (1 << WB_async_congested) | (1 << WB_sync_congested));
 		put_ldev(device);
 	}
 	s->dev_size = get_capacity(device->vdisk);
@@ -6507,9 +6512,9 @@ static int adm_del_resource(struct drbd_
 	drbd_debugfs_resource_cleanup(resource);
 	mutex_unlock(&resources_mutex);
 
-	timer_shutdown_sync(&resource->twopc_timer);
-	timer_shutdown_sync(&resource->peer_ack_timer);
-	timer_shutdown_sync(&resource->repost_up_to_date_timer);
+	del_timer_sync(&resource->twopc_timer);
+	del_timer_sync(&resource->peer_ack_timer);
+	del_timer_sync(&resource->repost_up_to_date_timer);
 	call_rcu(&resource->rcu, drbd_reclaim_resource);
 
 	mutex_lock(&notification_mutex);
@@ -7275,7 +7280,7 @@ int drbd_adm_rename_resource(struct sk_b
 	}
 	old_res_name = resource->name;
 	resource->name = new_res_name;
-	kvfree_rcu_mightsleep(old_res_name);
+	kvfree_rcu(old_res_name);
 
 	drbd_debugfs_resource_rename(resource, new_res_name);
 
--- drbd_main.c
+++ /tmp/cocci-output-1018120-746e98-drbd_main.c
@@ -1592,7 +1592,7 @@ int drbd_send_sizes(struct drbd_peer_dev
 			cpu_to_be32(bdev_alignment_offset(bdev));
 		p->qlim->io_min = cpu_to_be32(bdev_io_min(bdev));
 		p->qlim->io_opt = cpu_to_be32(bdev_io_opt(bdev));
-		p->qlim->discard_enabled = !!bdev_max_discard_sectors(bdev);
+		p->qlim->discard_enabled = !!bdev_get_queue(bdev)->limits.max_discard_sectors;
 		p->qlim->write_same_capable = 0;
 		put_ldev(device);
 	} else {
@@ -2340,7 +2340,7 @@ int drbd_send_dblock(struct drbd_peer_de
 	int digest_size = 0;
 	int err;
 	const unsigned s = req->net_rq_state[peer_device->node_id];
-	const enum req_op op = bio_op(req->master_bio);
+	const unsigned int op = bio_op(req->master_bio);
 
 	if (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {
 		trim = drbd_prepare_command(peer_device, sizeof(*trim), DATA_STREAM);
@@ -3148,7 +3148,7 @@ static void drbd_device_finalize_work_fn
 		device->bitmap = NULL;
 	}
 
-	put_disk(device->vdisk);
+	blk_cleanup_disk(device->vdisk);
 
 	kfree(device);
 
@@ -3965,7 +3965,7 @@ enum drbd_ret_code drbd_create_device(st
 	disk->first_minor = minor;
 	disk->minors = 1;
 	disk->fops = &drbd_ops;
-	disk->flags |= GENHD_FL_NO_PART;
+	disk->flags |= GENHD_FL_NO_PART_SCAN;
 	sprintf(disk->disk_name, "drbd%d", minor);
 	disk->private_data = device;
 
@@ -4109,7 +4109,7 @@ out_no_peer_device:
 out_no_bitmap:
 	__free_page(device->md_io.page);
 out_no_io_page:
-	put_disk(disk);
+	blk_cleanup_disk(disk);
 out_no_disk:
 	kref_put(&resource->kref, drbd_destroy_resource);
 	kref_debug_put(&resource->kref_debug, 4);
@@ -4152,7 +4152,7 @@ void drbd_unregister_device(struct drbd_
 	device->submit_conflict.wq = NULL;
 	destroy_workqueue(device->submit.wq);
 	device->submit.wq = NULL;
-	timer_shutdown_sync(&device->request_timer);
+	del_timer_sync(&device->request_timer);
 }
 
 void drbd_reclaim_device(struct rcu_head *rp)
@@ -4174,7 +4174,7 @@ void drbd_reclaim_device(struct rcu_head
 
 static void shutdown_connect_timer(struct drbd_connection *connection)
 {
-	if (timer_shutdown_sync(&connection->connect_timer)) {
+	if (del_timer_sync(&connection->connect_timer)) {
 		kref_debug_put(&connection->kref_debug, 11);
 		kref_put(&connection->kref, drbd_destroy_connection);
 	}
--- drbd_debugfs.c
+++ /tmp/cocci-output-1018120-c0ed25-drbd_debugfs.c
@@ -1853,6 +1853,27 @@ static const struct file_operations drbd
 
 static int drbd_compat_show(struct seq_file *m, void *ignored)
 {
+	seq_puts(m, "bio_split_to_limits__no_present\n");
+	seq_puts(m, "bio_alloc__no_has_4_params\n");
+	seq_puts(m, "bio_alloc_clone__no_present\n");
+	seq_puts(m, "blk_cleanup_disk__yes_present\n");
+	seq_puts(m, "submit_bio__no_returns_void\n");
+	seq_puts(m, "queue_flag_discard__yes_present\n");
+	seq_puts(m, "blk_opf_t__no_present\n");
+	seq_puts(m, "enum_req_op__no_present\n");
+	seq_puts(m, "bdi_congested__yes_present\n");
+	seq_puts(m, "fs_dax_get_by_bdev__no_takes_start_off_and_holder\n");
+	seq_puts(m, "fs_dax_get_by_bdev__no_takes_start_off\n");
+	seq_puts(m, "genhd_fl_no_part__no_present\n");
+	seq_puts(m, "bdev_max_discard_sectors__no_present\n");
+	seq_puts(m, "blk_queue_max_write_same_sectors__yes_present\n");
+	seq_puts(m, "blkdev_issue_discard__yes_takes_flags\n");
+	seq_puts(m, "bdev_discard_granularity__no_present\n");
+	seq_puts(m, "kvfree_rcu_mightsleep__no_present\n");
+	seq_puts(m, "get_random_u32_below__no_present\n");
+	seq_puts(m, "get_random_u32__no_present\n");
+	seq_puts(m, "sk_use_task_frag__no_present\n");
+	seq_puts(m, "timer_shutdown__no_present\n");
 	return 0;
 }
 
--- drbd_dax_pmem.c
+++ /tmp/cocci-output-1018120-872b1e-drbd_dax_pmem.c
@@ -58,9 +58,8 @@ int drbd_dax_open(struct drbd_backing_de
 {
 	struct dax_device *dax_dev;
 	int err;
-	u64 part_off;
 
-	dax_dev = fs_dax_get_by_bdev(bdev->md_bdev, &part_off, NULL, NULL);
+	dax_dev = fs_dax_get_by_bdev(bdev->md_bdev);
 	if (!dax_dev)
 		return -ENODEV;
 
--- drbd_bitmap.c
+++ /tmp/cocci-output-1018120-04c2c3-drbd_bitmap.c
@@ -1141,7 +1141,7 @@ static void bm_page_io_async(struct drbd
 	sector_t first_bm_sect;
 	sector_t on_disk_sector;
 	unsigned int len;
-	enum req_op op = ctx->flags & BM_AIO_READ ? REQ_OP_READ : REQ_OP_WRITE;
+	unsigned int op = ctx->flags & BM_AIO_READ ? REQ_OP_READ : REQ_OP_WRITE;
 
 	first_bm_sect = device->ldev->md.md_offset + device->ldev->md.bm_offset;
 	on_disk_sector = first_bm_sect + (((sector_t)page_nr) << (PAGE_SHIFT-SECTOR_SHIFT));
@@ -1185,14 +1185,15 @@ static void bm_page_io_async(struct drbd
 	} else
 		page = b->bm_pages[page_nr];
 
-	bio = bio_alloc_bioset(device->ldev->md_bdev, 1, op, GFP_NOIO,
-		&drbd_md_io_bio_set);
+	bio = bio_alloc_bioset(GFP_NOIO, 1, &drbd_md_io_bio_set);
+	bio_set_dev(bio, device->ldev->md_bdev);
 	bio->bi_iter.bi_sector = on_disk_sector;
 	/* bio_add_page of a single page to an empty bio will always succeed,
 	 * according to api.  Do we want to assert that? */
 	bio_add_page(bio, page, len, 0);
 	bio->bi_private = ctx;
 	bio->bi_end_io = drbd_bm_endio;
+	bio->bi_opf = op;
 
 	if (drbd_insert_fault(device, (op == REQ_OP_WRITE) ? DRBD_FAULT_MD_WR : DRBD_FAULT_MD_RD)) {
 		bio->bi_status = BLK_STS_IOERR;
--- drbd_actlog.c
+++ /tmp/cocci-output-1018120-d6b0b6-drbd_actlog.c
@@ -73,13 +73,13 @@ void wait_until_done_or_force_detached(s
 
 static int _drbd_md_sync_page_io(struct drbd_device *device,
 				 struct drbd_backing_dev *bdev,
-				 sector_t sector, enum req_op op)
+				 sector_t sector, unsigned int op)
 {
 	struct bio *bio;
 	/* we do all our meta data IO in aligned 4k blocks. */
 	const int size = 4096;
 	int err;
-	blk_opf_t op_flags = 0;
+	unsigned int op_flags = 0;
 
 	if ((op == REQ_OP_WRITE) && !test_bit(MD_NO_FUA, &device->flags))
 		op_flags |= REQ_FUA | REQ_PREFLUSH;
@@ -88,14 +88,15 @@ static int _drbd_md_sync_page_io(struct
 	device->md_io.done = 0;
 	device->md_io.error = -ENODEV;
 
-	bio = bio_alloc_bioset(bdev->md_bdev, 1, op | op_flags,
-		GFP_NOIO, &drbd_md_io_bio_set);
+	bio = bio_alloc_bioset(GFP_NOIO, 1, &drbd_md_io_bio_set);
+	bio_set_dev(bio, bdev->md_bdev);
 	bio->bi_iter.bi_sector = sector;
 	err = -EIO;
 	if (bio_add_page(bio, device->md_io.page, size, 0) != size)
 		goto out;
 	bio->bi_private = device;
 	bio->bi_end_io = drbd_md_endio;
+	bio->bi_opf = op | op_flags;
 
 	if (op != REQ_OP_WRITE && device->disk_state[NOW] == D_DISKLESS && device->ldev == NULL)
 		/* special case, drbd_md_read() during drbd_adm_attach(): no get_ldev */
@@ -124,7 +125,7 @@ static int _drbd_md_sync_page_io(struct
 }
 
 int drbd_md_sync_page_io(struct drbd_device *device, struct drbd_backing_dev *bdev,
-			 sector_t sector, enum req_op op)
+			 sector_t sector, unsigned int op)
 {
 	int err;
 	D_ASSERT(device, atomic_read(&device->md_io.in_use) == 1);
