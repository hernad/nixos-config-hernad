--- ./drbd_int.h
+++ /tmp/cocci-output-1013178-e7822c-drbd_int.h
@@ -443,7 +443,7 @@ struct drbd_peer_request {
 	};
 
 	struct drbd_page_chain_head page_chain;
-	blk_opf_t opf; /* to be used as bi_opf */
+	unsigned int opf; /* to be used as bi_opf */
 	atomic_t pending_bios;
 	struct drbd_interval i;
 	unsigned long flags;	/* see comments on ee flag bits below */
@@ -1960,7 +1960,7 @@ extern void do_submit(struct work_struct
 #define __drbd_make_request(d,b,k,j) __drbd_make_request(d,b,j)
 #endif
 extern void __drbd_make_request(struct drbd_device *, struct bio *, ktime_t, unsigned long);
-extern void drbd_submit_bio(struct bio *bio);
+extern blk_qc_t drbd_submit_bio(struct bio *bio);
 
 enum drbd_force_detach_flags {
 	DRBD_READ_ERROR,
@@ -2025,7 +2025,8 @@ extern void verify_progress(struct drbd_
 extern void *drbd_md_get_buffer(struct drbd_device *device, const char *intent);
 extern void drbd_md_put_buffer(struct drbd_device *device);
 extern int drbd_md_sync_page_io(struct drbd_device *device,
-		struct drbd_backing_dev *bdev, sector_t sector, enum req_op op);
+		struct drbd_backing_dev *bdev, sector_t sector,
+		unsigned int op);
 extern bool drbd_al_active(struct drbd_device *device, sector_t sector, unsigned int size);
 extern void drbd_ov_out_of_sync_found(struct drbd_peer_device *, sector_t, int);
 extern void wait_until_done_or_force_detached(struct drbd_device *device,
@@ -2179,7 +2180,7 @@ extern void drbd_last_resync_request(str
 
 static inline sector_t drbd_get_capacity(struct block_device *bdev)
 {
-	return bdev ? bdev_nr_sectors(bdev) : 0;
+	return bdev ? i_size_read(bdev->bd_inode) >> 9 : 0;
 }
 
 /* sets the number of 512 byte sectors of our virtual device */
--- drbd-headers/linux/genl_magic_struct.h
+++ /tmp/cocci-output-1013178-f6009d-genl_magic_struct.h
@@ -98,7 +98,7 @@ static inline int nla_put_u64_0pad(struc
 			nla_get_u64, nla_put_u64_0pad, false)
 #define __str_field(attr_nr, attr_flag, name, maxlen) \
 	__array(attr_nr, attr_flag, name, NLA_NUL_STRING, char, maxlen, \
-			nla_strscpy, nla_put, false)
+			nla_strlcpy, nla_put, false)
 #define __bin_field(attr_nr, attr_flag, name, maxlen) \
 	__array(attr_nr, attr_flag, name, NLA_BINARY, char, maxlen, \
 			nla_memcpy, nla_put, false)
--- drbd_transport_tcp.c
+++ /tmp/cocci-output-1013178-353561-drbd_transport_tcp.c
@@ -646,7 +646,7 @@ static int dtt_wait_for_connect(struct d
 	rcu_read_unlock();
 
 	timeo = connect_int * HZ;
-	timeo += get_random_u32_below(2) ? timeo / 7 : -timeo / 7; /* 28.5% random jitter */
+	timeo += (prandom_u32() % 2) ? timeo / 7 : -timeo / 7; /* 28.5% random jitter */
 
 retry:
 	timeo = wait_event_interruptible_timeout(listener->wait,
@@ -1112,7 +1112,7 @@ retry:
 				kernel_sock_shutdown(s, SHUT_RDWR);
 				sock_release(s);
 randomize:
-				if (get_random_u32_below(2))
+				if ((prandom_u32() % 2))
 					goto retry;
 			}
 		}
@@ -1134,9 +1134,6 @@ randomize:
 	dsocket->sk->sk_allocation = GFP_NOIO;
 	csocket->sk->sk_allocation = GFP_NOIO;
 
-	dsocket->sk->sk_use_task_frag = false;
-	csocket->sk->sk_use_task_frag = false;
-
 	dsocket->sk->sk_priority = TC_PRIO_INTERACTIVE_BULK;
 	csocket->sk->sk_priority = TC_PRIO_INTERACTIVE;
 
--- drbd_state.c
+++ /tmp/cocci-output-1013178-0f898e-drbd_state.c
@@ -4611,7 +4611,7 @@ long twopc_retry_timeout(struct drbd_res
 			retries = 5;
 		timeout = resource->res_opts.twopc_retry_timeout *
 			  HZ / 10 * connections * (1 << retries);
-		timeout = get_random_u32_below(timeout);
+		timeout = (prandom_u32() % timeout);
 	}
 	return timeout;
 }
@@ -4791,7 +4791,7 @@ change_cluster_wide_state(bool (*change)
 	}
 
 	do
-		reply->tid = get_random_u32();
+		reply->tid = prandom_u32();
 	while (!reply->tid);
 
 	request.tid = reply->tid;
@@ -5023,7 +5023,7 @@ retry:
 	*reply = (struct twopc_reply) { 0 };
 
 	do
-		reply->tid = get_random_u32();
+		reply->tid = prandom_u32();
 	while (!reply->tid);
 
 	request.tid = reply->tid;
--- drbd_sender.c
+++ /tmp/cocci-output-1013178-cc3e13-drbd_sender.c
@@ -323,7 +323,7 @@ void drbd_request_endio(struct bio *bio)
 
 	/* to avoid recursion in __req_mod */
 	if (unlikely(status)) {
-		enum req_op op = bio_op(bio);
+		unsigned int op = bio_op(bio);
 		if (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {
 			if (status == BLK_STS_NOTSUPP)
 				what = DISCARD_COMPLETED_NOTSUPP;
@@ -541,9 +541,9 @@ void drbd_csum_bio(struct crypto_shash *
 
 	bio_for_each_segment(bvec, bio, iter) {
 		u8 *src;
-		src = bvec_kmap_local(&bvec);
+		src = kmap_atomic(bvec.bv_page) + bvec.bv_offset;
 		crypto_shash_update(desc, src, bvec.bv_len);
-		kunmap_local(src);
+		kunmap_atomic(src);
 	}
 	crypto_shash_final(desc, digest);
 	shash_desc_zero(desc);
@@ -2648,7 +2648,8 @@ void drbd_rs_controller_reset(struct drb
 	atomic_set(&peer_device->device->rs_sect_ev, 0);  /* FIXME: ??? */
 	peer_device->rs_last_mk_req_kt = ktime_get();
 	peer_device->rs_in_flight = 0;
-	peer_device->rs_last_events = (int)part_stat_read_accum(disk->part0, sectors);
+	peer_device->rs_last_events = (int)part_stat_read_accum(&disk->part0,
+								sectors);
 
 	/* Updating the RCU protected object in place is necessary since
 	   this function gets called from atomic context.
--- drbd_req.c
+++ /tmp/cocci-output-1013178-210ff8-drbd_req.c
@@ -1411,7 +1411,7 @@ static bool remote_due_to_read_balancing
 		/* originally, this used the bdi congestion framework,
 		 * but that was removed in linux 5.18.
 		 * so just never report the lower device as congested. */
-		return false;
+		return bdi_read_congested(device->ldev->backing_bdev->bd_disk->queue->backing_dev_info);
 	case RB_LEAST_PENDING:
 		return atomic_read(&device->local_cnt) >
 			atomic_read(&peer_device->ap_pending_cnt) + atomic_read(&peer_device->rs_pending_cnt);
@@ -1809,7 +1809,8 @@ drbd_request_prepare(struct drbd_device
 	req->start_jif = bio_start_io_acct(req->master_bio);
 
 	if (get_ldev(device)) {
-		req->private_bio = bio_alloc_clone(device->ldev->backing_bdev, bio, GFP_NOIO, &drbd_io_bio_set);
+		req->private_bio = bio_clone_fast(bio, GFP_NOIO,
+						  &drbd_io_bio_set);
 		req->private_bio->bi_private = req;
 		req->private_bio->bi_end_io = drbd_request_endio;
 	}
@@ -2560,9 +2561,9 @@ static bool request_size_bad(struct drbd
  *                                           v
  *                                   Request state machine
  */
-void drbd_submit_bio(struct bio *bio)
+blk_qc_t drbd_submit_bio(struct bio *bio)
 {
-	struct drbd_device *device = bio->bi_bdev->bd_disk->private_data;
+	struct drbd_device *device = bio->bi_disk->private_data;
 #ifdef CONFIG_DRBD_TIMING_STATS
 	ktime_t start_kt;
 #endif
@@ -2571,17 +2572,15 @@ void drbd_submit_bio(struct bio *bio)
 	if (drbd_fail_request_early(device, bio)) {
 		bio->bi_status = BLK_STS_IOERR;
 		bio_endio(bio);
-		return;
+		return BLK_QC_T_NONE;
 	}
 
-	bio = bio_split_to_limits(bio);
-	if (!bio)
-		return;
+	blk_queue_split(&bio);
 
 	if (device->cached_err_io || request_size_bad(device, bio)) {
 		bio->bi_status = BLK_STS_IOERR;
 		bio_endio(bio);
-		return;
+		return BLK_QC_T_NONE;
 	}
 
 	/* This is both an optimization: READ of size 0, nothing to do
@@ -2593,13 +2592,14 @@ void drbd_submit_bio(struct bio *bio)
 	if (bio_op(bio) == REQ_OP_READ && bio->bi_iter.bi_size == 0) {
 		WARN_ONCE(1, "size zero read from upper layers");
 		bio_endio(bio);
-		return;
+		return BLK_QC_T_NONE;
 	}
 
 	ktime_get_accounting(start_kt);
 	start_jif = jiffies;
 
 	__drbd_make_request(device, bio, start_kt, start_jif);
+	return BLK_QC_T_NONE;
 }
 
 static unsigned long time_min_in_future(unsigned long now,
--- drbd_receiver.c
+++ /tmp/cocci-output-1013178-200ec5-drbd_receiver.c
@@ -1354,8 +1354,7 @@ static void one_flush_endio(struct bio *
 
 static void submit_one_flush(struct drbd_device *device, struct issue_flush_context *ctx)
 {
-	struct bio *bio = bio_alloc(device->ldev->backing_bdev, 0,
-			REQ_OP_WRITE | REQ_PREFLUSH, GFP_NOIO);
+	struct bio *bio = bio_alloc(GFP_NOIO, 0);
 	struct one_flush_context *octx = kmalloc(sizeof(*octx), GFP_NOIO);
 
 	if (!octx) {
@@ -1374,12 +1373,14 @@ static void submit_one_flush(struct drbd
 
 	octx->device = device;
 	octx->ctx = ctx;
+	bio_set_dev(bio, device->ldev->backing_bdev);
 	bio->bi_private = octx;
 	bio->bi_end_io = one_flush_endio;
 
 	device->flush_jif = jiffies;
 	set_bit(FLUSH_PENDING, &device->flags);
 	atomic_inc(&ctx->pending);
+	bio->bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;
 	submit_bio(bio);
 }
 
@@ -1722,7 +1723,8 @@ int drbd_issue_discard_or_zero_out(struc
 	granularity = max(q->limits.discard_granularity >> 9, 1U);
 	alignment = (bdev_discard_alignment(bdev) >> 9) % granularity;
 
-	max_discard_sectors = min(bdev_max_discard_sectors(bdev), (1U << 22));
+	max_discard_sectors = min(bdev_get_queue(bdev)->limits.max_discard_sectors,
+				  (1U << 22));
 	max_discard_sectors -= max_discard_sectors % granularity;
 	if (unlikely(!max_discard_sectors))
 		goto zero_out;
@@ -1746,7 +1748,8 @@ int drbd_issue_discard_or_zero_out(struc
 		start = tmp;
 	}
 	while (nr_sectors >= max_discard_sectors) {
-		err |= blkdev_issue_discard(bdev, start, max_discard_sectors, GFP_NOIO);
+		err |= blkdev_issue_discard(bdev, start, max_discard_sectors,
+					    GFP_NOIO, 0);
 		nr_sectors -= max_discard_sectors;
 		start += max_discard_sectors;
 	}
@@ -1758,7 +1761,8 @@ int drbd_issue_discard_or_zero_out(struc
 		nr = nr_sectors;
 		nr -= (unsigned int)nr % granularity;
 		if (nr) {
-			err |= blkdev_issue_discard(bdev, start, nr, GFP_NOIO);
+			err |= blkdev_issue_discard(bdev, start, nr, GFP_NOIO,
+						    0);
 			nr_sectors -= nr;
 			start += nr;
 		}
@@ -1776,7 +1780,7 @@ static bool can_do_reliable_discards(str
 	struct disk_conf *dc;
 	bool can_do;
 
-	if (!bdev_max_discard_sectors(device->ldev->backing_bdev))
+	if (!bdev_get_queue(device->ldev->backing_bdev)->limits.max_discard_sectors)
 		return false;
 
 	rcu_read_lock();
@@ -1882,12 +1886,13 @@ next_bio:
 
 	/* we special case some flags in the multi-bio case, see below
 	 * (REQ_PREFLUSH, or BIO_RW_BARRIER in older kernels) */
-	bio = bio_alloc(device->ldev->backing_bdev, nr_pages, peer_req->opf,
-			GFP_NOIO);
+	bio = bio_alloc(GFP_NOIO, nr_pages);
+	bio_set_dev(bio, device->ldev->backing_bdev);
 	/* > peer_req->i.sector, unless this is the first bio */
 	bio->bi_iter.bi_sector = sector;
 	bio->bi_private = peer_req;
 	bio->bi_end_io = drbd_peer_request_endio;
+	bio->bi_opf = peer_req->opf;
 
 	bio->bi_next = bios;
 	bios = bio;
@@ -2278,10 +2283,10 @@ static int recv_dless_read(struct drbd_p
 	D_ASSERT(peer_device->device, sector == bio->bi_iter.bi_sector);
 
 	bio_for_each_segment(bvec, bio, iter) {
-		void *mapped = bvec_kmap_local(&bvec);
+		void *mapped = kmap(bvec.bv_page) + bvec.bv_offset;
 		expect = min_t(int, data_size, bvec.bv_len);
 		err = drbd_recv_into(peer_device->connection, mapped, expect);
-		kunmap_local(mapped);
+		kunmap(bvec.bv_page);
 		if (err)
 			return err;
 		data_size -= expect;
@@ -3074,7 +3079,7 @@ static int wait_for_and_update_peer_seq(
 	return ret;
 }
 
-static enum req_op wire_flags_to_bio_op(u32 dpf)
+static unsigned int wire_flags_to_bio_op(u32 dpf)
 {
 	if (dpf & DP_ZEROES)
 		return REQ_OP_WRITE_ZEROES;
@@ -3084,9 +3089,9 @@ static enum req_op wire_flags_to_bio_op(
 }
 
 /* see also bio_flags_to_wire() */
-static blk_opf_t wire_flags_to_bio(struct drbd_connection *connection, u32 dpf)
+static unsigned int wire_flags_to_bio(struct drbd_connection *connection, u32 dpf)
 {
-	blk_opf_t opf = wire_flags_to_bio_op(dpf) |
+	unsigned int opf = wire_flags_to_bio_op(dpf) |
 		(dpf & DP_RW_SYNC ? REQ_SYNC : 0);
 
 	/* we used to communicate one bit only in older DRBD */
@@ -3628,7 +3633,7 @@ bool drbd_rs_c_min_rate_throttle(struct
 	if (c_min_rate == 0)
 		return false;
 
-	curr_events = (int)part_stat_read_accum(disk->part0, sectors)
+	curr_events = (int)part_stat_read_accum(&disk->part0, sectors)
 		- atomic_read(&device->rs_sect_ev);
 
 	if (atomic_read(&device->ap_actlog_cnt) || curr_events - peer_device->rs_last_events > 64) {
@@ -5488,7 +5493,7 @@ static int receive_protocol(struct drbd_
 		drbd_info(connection, "peer data-integrity-alg: %s\n",
 			  integrity_alg[0] ? integrity_alg : "(none)");
 
-	kvfree_rcu_mightsleep(old_net_conf);
+	kvfree_rcu(old_net_conf);
 	return 0;
 
 disconnect_rcu_unlock:
@@ -5949,7 +5954,7 @@ static int receive_sizes(struct drbd_con
 			new_disk_conf->disk_size = p_usize;
 
 			rcu_assign_pointer(device->ldev->disk_conf, new_disk_conf);
-			kvfree_rcu_mightsleep(old_disk_conf);
+			kvfree_rcu(old_disk_conf);
 
 			drbd_info(peer_device, "Peer sets u_size to %llu sectors (old: %llu)\n",
 				 (unsigned long long)p_usize, (unsigned long long)my_usize);
@@ -7191,7 +7196,7 @@ drbd_commit_size_change(struct drbd_devi
 		new_disk_conf->disk_size = tr->user_size;
 
 		rcu_assign_pointer(device->ldev->disk_conf, new_disk_conf);
-		kvfree_rcu_mightsleep(old_disk_conf);
+		kvfree_rcu(old_disk_conf);
 
 		drbd_info(device, "New u_size %llu sectors\n",
 			  (unsigned long long)tr->user_size);
@@ -8981,8 +8986,8 @@ void drbd_process_rs_discards(struct drb
 		 * than DRBD_MAX_RS_DISCARD_SIZE, then allow merging up to a size of
 		 * DRBD_MAX_RS_DISCARD_SIZE.
 		 */
-		align = max(DRBD_MAX_RS_DISCARD_SIZE, bdev_discard_granularity(
-					device->ldev->backing_bdev)) >> SECTOR_SHIFT;
+		align = max(DRBD_MAX_RS_DISCARD_SIZE,
+		            (device->ldev->backing_bdev->bd_disk->queue->limits.discard_granularity ? : 512)) >> SECTOR_SHIFT;
 		put_ldev(device);
 	}
 
@@ -9623,7 +9628,7 @@ static void conn_disconnect(struct drbd_
 	atomic_set(&connection->current_epoch->epoch_size, 0);
 	connection->send.seen_any_write_yet = false;
 	connection->send.current_dagtag_sector =
-		resource->dagtag_sector - ((BIO_MAX_VECS << PAGE_SHIFT) >> SECTOR_SHIFT) - 1;
+		resource->dagtag_sector - ((BIO_MAX_PAGES << PAGE_SHIFT) >> SECTOR_SHIFT) - 1;
 	connection->current_epoch->oldest_unconfirmed_peer_req = NULL;
 
 	/* Indicate that last_dagtag_sector may no longer be up-to-date. We
--- drbd_nl.c
+++ /tmp/cocci-output-1013178-1c5022-drbd_nl.c
@@ -1505,7 +1505,8 @@ void drbd_set_my_capacity(struct drbd_de
 {
 	char ppb[10];
 
-	set_capacity_and_notify(device->vdisk, size);
+	set_capacity(device->vdisk, size);
+	revalidate_disk_size(device->vdisk, false);
 
 	drbd_info(device, "size = %s (%llu KB)\n",
 		ppsize(ppb, size>>1), (unsigned long long)size>>1);
@@ -1945,7 +1946,7 @@ static void decide_on_discard_support(st
 	struct request_queue *q = device->rq_queue;
 	unsigned int max_discard_sectors;
 
-	if (bdev && !bdev_max_discard_sectors(bdev->backing_bdev))
+	if (bdev && !bdev_get_queue(bdev->backing_bdev)->limits.max_discard_sectors)
 		goto not_supported;
 
 	if (!(common_connection_features(device->resource) & DRBD_FF_TRIM)) {
@@ -1963,6 +1964,7 @@ static void decide_on_discard_support(st
 	 * topology on all peers.
 	 */
 	blk_queue_discard_granularity(q, 512);
+	blk_queue_flag_set(QUEUE_FLAG_DISCARD, q);
 	max_discard_sectors = drbd_max_discard_sectors(device->resource);
 	blk_queue_max_discard_sectors(q, max_discard_sectors);
 	blk_queue_max_write_zeroes_sectors(q, max_discard_sectors);
@@ -1970,6 +1972,7 @@ static void decide_on_discard_support(st
 
 not_supported:
 	blk_queue_discard_granularity(q, 0);
+	blk_queue_flag_clear(QUEUE_FLAG_DISCARD, q);
 	blk_queue_max_discard_sectors(q, 0);
 }
 
@@ -1995,6 +1998,7 @@ static void fixup_discard_support(struct
 
 	if (discard_granularity > max_discard) {
 		blk_queue_discard_granularity(q, 0);
+		blk_queue_flag_clear(QUEUE_FLAG_DISCARD, q);
 		blk_queue_max_discard_sectors(q, 0);
 	}
 }
@@ -2036,10 +2040,11 @@ void drbd_reconsider_queue_parameters(st
 	if (bdev) {
 		b = bdev->backing_bdev->bd_disk->queue;
 		blk_stack_limits(&common_limits, &b->limits, 0);
-		disk_update_readahead(device->vdisk);
+		blk_queue_update_readahead(q);
 	}
 	q->limits = common_limits;
 	blk_queue_max_hw_sectors(q, common_limits.max_hw_sectors);
+	blk_queue_max_write_same_sectors(q, 0);
 	decide_on_discard_support(device, bdev);
 
 	fixup_write_zeroes(device, q);
@@ -2134,7 +2139,7 @@ static void sanitize_disk_conf(struct dr
 	if (disk_conf->al_extents > drbd_al_extents_max(nbc))
 		disk_conf->al_extents = drbd_al_extents_max(nbc);
 
-	if (!bdev_max_discard_sectors(bdev)) {
+	if (!bdev_get_queue(bdev)->limits.max_discard_sectors) {
 		if (disk_conf->rs_discard_granularity) {
 			disk_conf->rs_discard_granularity = 0; /* disable feature */
 			drbd_info(device, "rs_discard_granularity feature disabled\n");
@@ -2152,8 +2157,8 @@ static void sanitize_disk_conf(struct dr
 	if (disk_conf->rs_discard_granularity) {
 		unsigned int new_discard_granularity =
 			disk_conf->rs_discard_granularity;
-		unsigned int discard_sectors = bdev_max_discard_sectors(bdev);
-		unsigned int discard_granularity = bdev_discard_granularity(bdev);
+		unsigned int discard_sectors = bdev_get_queue(bdev)->limits.max_discard_sectors;
+		unsigned int discard_granularity = (bdev->bd_disk->queue->limits.discard_granularity ? : 512);
 
 		/* should be at least the discard_granularity of the bdev,
 		 * and preferably a multiple (or the backend won't be able to
@@ -2302,7 +2307,7 @@ int drbd_adm_disk_opts(struct sk_buff *s
 			drbd_send_sync_param(peer_device);
 	}
 
-	kvfree_rcu_mightsleep(old_disk_conf);
+	kvfree_rcu(old_disk_conf);
 	mod_timer(&device->request_timer, jiffies + HZ);
 	goto success;
 
@@ -3727,7 +3732,7 @@ int drbd_adm_net_opts(struct sk_buff *sk
 
 	mutex_unlock(&connection->mutex[DATA_STREAM]);
 	mutex_unlock(&connection->resource->conf_update);
-	kvfree_rcu_mightsleep(old_net_conf);
+	kvfree_rcu(old_net_conf);
 
 	if (connection->cstate[NOW] >= C_CONNECTED) {
 		struct drbd_peer_device *peer_device;
@@ -3859,7 +3864,7 @@ int drbd_adm_peer_device_opts(struct sk_
 
 	rcu_assign_pointer(peer_device->conf, new_peer_device_conf);
 
-	kvfree_rcu_mightsleep(old_peer_device_conf);
+	kvfree_rcu(old_peer_device_conf);
 	kfree(old_plan);
 
 	/* No need to call drbd_send_sync_param() here. The values in
@@ -4879,7 +4884,7 @@ int drbd_adm_resize(struct sk_buff *skb,
 		new_disk_conf->disk_size = (sector_t)rs.resize_size;
 		rcu_assign_pointer(device->ldev->disk_conf, new_disk_conf);
 		mutex_unlock(&device->resource->conf_update);
-		kvfree_rcu_mightsleep(old_disk_conf);
+		kvfree_rcu(old_disk_conf);
 		new_disk_conf = NULL;
 	}
 
@@ -5489,7 +5494,8 @@ static void device_to_statistics(struct
 		/* originally, this used the bdi congestion framework,
 		 * but that was removed in linux 5.18.
 		 * so just never report the lower device as congested. */
-		s->dev_lower_blocked = false;
+		s->dev_lower_blocked = bdi_congested(device->ldev->backing_bdev->bd_disk->queue->backing_dev_info,
+						     (1 << WB_async_congested) | (1 << WB_sync_congested));
 		put_ldev(device);
 	}
 	s->dev_size = get_capacity(device->vdisk);
@@ -6507,9 +6513,9 @@ static int adm_del_resource(struct drbd_
 	drbd_debugfs_resource_cleanup(resource);
 	mutex_unlock(&resources_mutex);
 
-	timer_shutdown_sync(&resource->twopc_timer);
-	timer_shutdown_sync(&resource->peer_ack_timer);
-	timer_shutdown_sync(&resource->repost_up_to_date_timer);
+	del_timer_sync(&resource->twopc_timer);
+	del_timer_sync(&resource->peer_ack_timer);
+	del_timer_sync(&resource->repost_up_to_date_timer);
 	call_rcu(&resource->rcu, drbd_reclaim_resource);
 
 	mutex_lock(&notification_mutex);
@@ -7275,7 +7281,7 @@ int drbd_adm_rename_resource(struct sk_b
 	}
 	old_res_name = resource->name;
 	resource->name = new_res_name;
-	kvfree_rcu_mightsleep(old_res_name);
+	kvfree_rcu(old_res_name);
 
 	drbd_debugfs_resource_rename(resource, new_res_name);
 
--- drbd_main.c
+++ /tmp/cocci-output-1013178-ff0c90-drbd_main.c
@@ -1592,7 +1592,7 @@ int drbd_send_sizes(struct drbd_peer_dev
 			cpu_to_be32(bdev_alignment_offset(bdev));
 		p->qlim->io_min = cpu_to_be32(bdev_io_min(bdev));
 		p->qlim->io_opt = cpu_to_be32(bdev_io_opt(bdev));
-		p->qlim->discard_enabled = !!bdev_max_discard_sectors(bdev);
+		p->qlim->discard_enabled = !!bdev_get_queue(bdev)->limits.max_discard_sectors;
 		p->qlim->write_same_capable = 0;
 		put_ldev(device);
 	} else {
@@ -2340,7 +2340,7 @@ int drbd_send_dblock(struct drbd_peer_de
 	int digest_size = 0;
 	int err;
 	const unsigned s = req->net_rq_state[peer_device->node_id];
-	const enum req_op op = bio_op(req->master_bio);
+	const unsigned int op = bio_op(req->master_bio);
 
 	if (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {
 		trim = drbd_prepare_command(peer_device, sizeof(*trim), DATA_STREAM);
@@ -2848,7 +2848,10 @@ void drbd_fsync_device(struct drbd_devic
 {
 	struct drbd_resource *resource = device->resource;
 
-	sync_blockdev(device->vdisk->part0);
+	struct block_device *bdev = bdget_disk(device->vdisk, 0);
+	if (bdev)
+		sync_blockdev(bdev);
+	bdput(bdev);
 	/* Prevent writes occurring after demotion, at least
 	 * the writes already submitted in this context. This
 	 * covers the case where DRBD auto-demotes on release,
@@ -3669,7 +3672,7 @@ struct drbd_connection *drbd_create_conn
 	connection->send.current_epoch_nr = 0;
 	connection->send.current_epoch_writes = 0;
 	connection->send.current_dagtag_sector =
-		resource->dagtag_sector - ((BIO_MAX_VECS << PAGE_SHIFT) >> SECTOR_SHIFT) - 1;
+		resource->dagtag_sector - ((BIO_MAX_PAGES << PAGE_SHIFT) >> SECTOR_SHIFT) - 1;
 
 	connection->cstate[NOW] = C_STANDALONE;
 	connection->peer_role[NOW] = R_UNKNOWN;
@@ -3886,6 +3889,7 @@ enum drbd_ret_code drbd_create_device(st
 	struct drbd_resource *resource = adm_ctx->resource;
 	struct drbd_connection *connection;
 	struct drbd_device *device;
+	struct request_queue *q;
 	struct drbd_peer_device *peer_device, *tmp_peer_device;
 	struct gendisk *disk;
 	LIST_HEAD(peer_devices);
@@ -3952,20 +3956,24 @@ enum drbd_ret_code drbd_create_device(st
 
 	init_rwsem(&device->uuid_sem);
 
-	disk = blk_alloc_disk(NUMA_NO_NODE);
+	q = blk_alloc_queue(NUMA_NO_NODE);
+	if (!q) {
+		goto out_no_q;
+	}
+	device->rq_queue = q;
+	disk = alloc_disk(1);
 	if (!disk)
 		goto out_no_disk;
 
 	INIT_WORK(&device->ldev_destroy_work, drbd_ldev_destroy);
 
 	device->vdisk = disk;
-	device->rq_queue = disk->queue;
 
 	disk->major = DRBD_MAJOR;
 	disk->first_minor = minor;
-	disk->minors = 1;
+	disk->queue = q;
 	disk->fops = &drbd_ops;
-	disk->flags |= GENHD_FL_NO_PART;
+	disk->flags |= GENHD_FL_NO_PART_SCAN;
 	sprintf(disk->disk_name, "drbd%d", minor);
 	disk->private_data = device;
 
@@ -4052,9 +4060,7 @@ enum drbd_ret_code drbd_create_device(st
 		goto out_remove_peer_device;
 	}
 
-	err = add_disk(disk);
-	if (err)
-		goto out_destroy_submitter;
+	add_disk(disk);
 	device->have_quorum[OLD] =
 	device->have_quorum[NEW] =
 		(resource->res_opts.quorum == QOU_OFF);
@@ -4071,9 +4077,6 @@ enum drbd_ret_code drbd_create_device(st
 	*p_device = device;
 	return NO_ERROR;
 
-out_destroy_submitter:
-	destroy_workqueue(device->submit.wq);
-	device->submit.wq = NULL;
 out_remove_peer_device:
 	list_splice_init_rcu(&device->peer_devices, &tmp, synchronize_rcu);
 	list_for_each_entry_safe(peer_device, tmp_peer_device, &tmp, peer_devices) {
@@ -4111,6 +4114,8 @@ out_no_bitmap:
 out_no_io_page:
 	put_disk(disk);
 out_no_disk:
+	blk_cleanup_queue(q);
+out_no_q:
 	kref_put(&resource->kref, drbd_destroy_resource);
 	kref_debug_put(&resource->kref_debug, 4);
 		/* kref debugging wants an extra put, see has_refs() */
@@ -4152,7 +4157,7 @@ void drbd_unregister_device(struct drbd_
 	device->submit_conflict.wq = NULL;
 	destroy_workqueue(device->submit.wq);
 	device->submit.wq = NULL;
-	timer_shutdown_sync(&device->request_timer);
+	del_timer_sync(&device->request_timer);
 }
 
 void drbd_reclaim_device(struct rcu_head *rp)
@@ -4174,7 +4179,7 @@ void drbd_reclaim_device(struct rcu_head
 
 static void shutdown_connect_timer(struct drbd_connection *connection)
 {
-	if (timer_shutdown_sync(&connection->connect_timer)) {
+	if (del_timer_sync(&connection->connect_timer)) {
 		kref_debug_put(&connection->kref_debug, 11);
 		kref_put(&connection->kref, drbd_destroy_connection);
 	}
--- drbd_debugfs.c
+++ /tmp/cocci-output-1013178-e723b6-drbd_debugfs.c
@@ -1853,6 +1853,41 @@ static const struct file_operations drbd
 
 static int drbd_compat_show(struct seq_file *m, void *ignored)
 {
+	seq_puts(m, "bio_split_to_limits__no_present\n");
+	seq_puts(m, "bio_alloc__no_has_4_params\n");
+	seq_puts(m, "bio_alloc_clone__no_present\n");
+	seq_puts(m, "bio_bi_bdev__no_present\n");
+	seq_puts(m, "bvec_kmap_local__no_present\n");
+	seq_puts(m, "blk_alloc_disk__no_present\n");
+	seq_puts(m, "submit_bio__no_returns_void\n");
+	seq_puts(m, "queue_flag_discard__yes_present\n");
+	seq_puts(m, "blk_opf_t__no_present\n");
+	seq_puts(m, "enum_req_op__no_present\n");
+	seq_puts(m, "bdi_congested__yes_present\n");
+	seq_puts(m, "disk_update_readahead__no_present\n");
+	seq_puts(m, "struct_gendisk__no_has_backing_dev_info\n");
+	seq_puts(m, "set_capacity_and_notify__no_present\n");
+	seq_puts(m, "nla_strscpy__no_present\n");
+	seq_puts(m, "part_stat_read__no_takes_block_device\n");
+	seq_puts(m, "bdgrab__yes_present\n");
+	seq_puts(m, "gendisk_part0__no_is_block_device\n");
+	seq_puts(m, "bio_max_vecs__no_present\n");
+	seq_puts(m, "fs_dax_get_by_bdev__no_takes_start_off_and_holder\n");
+	seq_puts(m, "fs_dax_get_by_bdev__no_takes_start_off\n");
+	seq_puts(m, "add_disk__no_returns_int\n");
+	seq_puts(m, "bdev_nr_sectors__no_present\n");
+	seq_puts(m, "genhd_fl_no_part__no_present\n");
+	seq_puts(m, "list_is_first__no_present\n");
+	seq_puts(m, "dax_direct_access__no_takes_mode\n");
+	seq_puts(m, "bdev_max_discard_sectors__no_present\n");
+	seq_puts(m, "blk_queue_max_write_same_sectors__yes_present\n");
+	seq_puts(m, "blkdev_issue_discard__yes_takes_flags\n");
+	seq_puts(m, "bdev_discard_granularity__no_present\n");
+	seq_puts(m, "kvfree_rcu_mightsleep__no_present\n");
+	seq_puts(m, "get_random_u32_below__no_present\n");
+	seq_puts(m, "get_random_u32__no_present\n");
+	seq_puts(m, "sk_use_task_frag__no_present\n");
+	seq_puts(m, "timer_shutdown__no_present\n");
 	return 0;
 }
 
--- drbd_dax_pmem.c
+++ /tmp/cocci-output-1013178-e58904-drbd_dax_pmem.c
@@ -39,7 +39,7 @@ static int map_superblock_for_dax(struct
 	int id;
 
 	id = dax_read_lock();
-	len = dax_direct_access(dax_dev, pgoff, want, DAX_ACCESS, &kaddr, &pfn_unused);
+	len = dax_direct_access(dax_dev, pgoff, want, &kaddr, &pfn_unused);
 	dax_read_unlock(id);
 
 	if (len < want)
@@ -58,9 +58,8 @@ int drbd_dax_open(struct drbd_backing_de
 {
 	struct dax_device *dax_dev;
 	int err;
-	u64 part_off;
 
-	dax_dev = fs_dax_get_by_bdev(bdev->md_bdev, &part_off, NULL, NULL);
+	dax_dev = fs_dax_get_by_bdev(bdev->md_bdev);
 	if (!dax_dev)
 		return -ENODEV;
 
@@ -97,7 +96,7 @@ int drbd_dax_map(struct drbd_backing_dev
 	int id;
 
 	id = dax_read_lock();
-	len = dax_direct_access(dax_dev, pgoff, want, DAX_ACCESS, &kaddr, &pfn_unused);
+	len = dax_direct_access(dax_dev, pgoff, want, &kaddr, &pfn_unused);
 	dax_read_unlock(id);
 
 	if (len < want)
--- drbd_bitmap.c
+++ /tmp/cocci-output-1013178-917774-drbd_bitmap.c
@@ -1141,7 +1141,7 @@ static void bm_page_io_async(struct drbd
 	sector_t first_bm_sect;
 	sector_t on_disk_sector;
 	unsigned int len;
-	enum req_op op = ctx->flags & BM_AIO_READ ? REQ_OP_READ : REQ_OP_WRITE;
+	unsigned int op = ctx->flags & BM_AIO_READ ? REQ_OP_READ : REQ_OP_WRITE;
 
 	first_bm_sect = device->ldev->md.md_offset + device->ldev->md.bm_offset;
 	on_disk_sector = first_bm_sect + (((sector_t)page_nr) << (PAGE_SHIFT-SECTOR_SHIFT));
@@ -1185,14 +1185,15 @@ static void bm_page_io_async(struct drbd
 	} else
 		page = b->bm_pages[page_nr];
 
-	bio = bio_alloc_bioset(device->ldev->md_bdev, 1, op, GFP_NOIO,
-		&drbd_md_io_bio_set);
+	bio = bio_alloc_bioset(GFP_NOIO, 1, &drbd_md_io_bio_set);
+	bio_set_dev(bio, device->ldev->md_bdev);
 	bio->bi_iter.bi_sector = on_disk_sector;
 	/* bio_add_page of a single page to an empty bio will always succeed,
 	 * according to api.  Do we want to assert that? */
 	bio_add_page(bio, page, len, 0);
 	bio->bi_private = ctx;
 	bio->bi_end_io = drbd_bm_endio;
+	bio->bi_opf = op;
 
 	if (drbd_insert_fault(device, (op == REQ_OP_WRITE) ? DRBD_FAULT_MD_WR : DRBD_FAULT_MD_RD)) {
 		bio->bi_status = BLK_STS_IOERR;
--- drbd_actlog.c
+++ /tmp/cocci-output-1013178-f7c06a-drbd_actlog.c
@@ -73,13 +73,13 @@ void wait_until_done_or_force_detached(s
 
 static int _drbd_md_sync_page_io(struct drbd_device *device,
 				 struct drbd_backing_dev *bdev,
-				 sector_t sector, enum req_op op)
+				 sector_t sector, unsigned int op)
 {
 	struct bio *bio;
 	/* we do all our meta data IO in aligned 4k blocks. */
 	const int size = 4096;
 	int err;
-	blk_opf_t op_flags = 0;
+	unsigned int op_flags = 0;
 
 	if ((op == REQ_OP_WRITE) && !test_bit(MD_NO_FUA, &device->flags))
 		op_flags |= REQ_FUA | REQ_PREFLUSH;
@@ -88,14 +88,15 @@ static int _drbd_md_sync_page_io(struct
 	device->md_io.done = 0;
 	device->md_io.error = -ENODEV;
 
-	bio = bio_alloc_bioset(bdev->md_bdev, 1, op | op_flags,
-		GFP_NOIO, &drbd_md_io_bio_set);
+	bio = bio_alloc_bioset(GFP_NOIO, 1, &drbd_md_io_bio_set);
+	bio_set_dev(bio, bdev->md_bdev);
 	bio->bi_iter.bi_sector = sector;
 	err = -EIO;
 	if (bio_add_page(bio, device->md_io.page, size, 0) != size)
 		goto out;
 	bio->bi_private = device;
 	bio->bi_end_io = drbd_md_endio;
+	bio->bi_opf = op | op_flags;
 
 	if (op != REQ_OP_WRITE && device->disk_state[NOW] == D_DISKLESS && device->ldev == NULL)
 		/* special case, drbd_md_read() during drbd_adm_attach(): no get_ldev */
@@ -124,7 +125,7 @@ static int _drbd_md_sync_page_io(struct
 }
 
 int drbd_md_sync_page_io(struct drbd_device *device, struct drbd_backing_dev *bdev,
-			 sector_t sector, enum req_op op)
+			 sector_t sector, unsigned int op)
 {
 	int err;
 	D_ASSERT(device, atomic_read(&device->md_io.in_use) == 1);
